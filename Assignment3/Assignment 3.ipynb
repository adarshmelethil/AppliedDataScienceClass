{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 3** - 100 marks\n",
    "\n",
    "**Due:** November 1st, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you are done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem1: Heart disease diagnosis (75 points)\n",
    "\n",
    "In this problem you are presented with a tabular dataset with 13 attributes that are thought to be good indicators of a heart diseases. We are going to train a number of clustering and binary classification algorithms on these data and see which ones perform better. \n",
    "\n",
    "The dataset is stored in CSV format. Use pandas `read_csv` to load it in python. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>283</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>243</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>417</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>234</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>319</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>315</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>126</td>\n",
       "      <td>218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>207</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>318</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>152</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "5     57    1   0       140   192    0        1      148      0      0.4   \n",
       "6     56    0   1       140   294    0        0      153      0      1.3   \n",
       "7     44    1   1       120   263    0        1      173      0      0.0   \n",
       "8     52    1   2       172   199    1        1      162      0      0.5   \n",
       "9     57    1   2       150   168    0        1      174      0      1.6   \n",
       "10    54    1   0       140   239    0        1      160      0      1.2   \n",
       "11    48    0   2       130   275    0        1      139      0      0.2   \n",
       "12    49    1   1       130   266    0        1      171      0      0.6   \n",
       "13    64    1   3       110   211    0        0      144      1      1.8   \n",
       "14    58    0   3       150   283    1        0      162      0      1.0   \n",
       "15    50    0   2       120   219    0        1      158      0      1.6   \n",
       "16    58    0   2       120   340    0        1      172      0      0.0   \n",
       "17    66    0   3       150   226    0        1      114      0      2.6   \n",
       "18    43    1   0       150   247    0        1      171      0      1.5   \n",
       "19    69    0   3       140   239    0        1      151      0      1.8   \n",
       "20    59    1   0       135   234    0        1      161      0      0.5   \n",
       "21    44    1   2       130   233    0        1      179      1      0.4   \n",
       "22    42    1   0       140   226    0        1      178      0      0.0   \n",
       "23    61    1   2       150   243    1        1      137      1      1.0   \n",
       "24    40    1   3       140   199    0        1      178      1      1.4   \n",
       "25    71    0   1       160   302    0        1      162      0      0.4   \n",
       "26    59    1   2       150   212    1        1      157      0      1.6   \n",
       "27    51    1   2       110   175    0        1      123      0      0.6   \n",
       "28    65    0   2       140   417    1        0      157      0      0.8   \n",
       "29    53    1   2       130   197    1        0      152      0      1.2   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "273   58    1   0       100   234    0        1      156      0      0.1   \n",
       "274   47    1   0       110   275    0        0      118      1      1.0   \n",
       "275   52    1   0       125   212    0        1      168      0      1.0   \n",
       "276   58    1   0       146   218    0        1      105      0      2.0   \n",
       "277   57    1   1       124   261    0        1      141      0      0.3   \n",
       "278   58    0   1       136   319    1        0      152      0      0.0   \n",
       "279   61    1   0       138   166    0        0      125      1      3.6   \n",
       "280   42    1   0       136   315    0        1      125      1      1.8   \n",
       "281   52    1   0       128   204    1        1      156      1      1.0   \n",
       "282   59    1   2       126   218    1        1      134      0      2.2   \n",
       "283   40    1   0       152   223    0        1      181      0      0.0   \n",
       "284   61    1   0       140   207    0        0      138      1      1.9   \n",
       "285   46    1   0       140   311    0        1      120      1      1.8   \n",
       "286   59    1   3       134   204    0        1      162      0      0.8   \n",
       "287   57    1   1       154   232    0        0      164      0      0.0   \n",
       "288   57    1   0       110   335    0        1      143      1      3.0   \n",
       "289   55    0   0       128   205    0        2      130      1      2.0   \n",
       "290   61    1   0       148   203    0        1      161      0      0.0   \n",
       "291   58    1   0       114   318    0        2      140      0      4.4   \n",
       "292   58    0   0       170   225    1        0      146      1      2.8   \n",
       "293   67    1   2       152   212    0        0      150      0      0.8   \n",
       "294   44    1   0       120   169    0        1      144      1      2.8   \n",
       "295   63    1   0       140   187    0        0      144      1      4.0   \n",
       "296   63    0   0       124   197    0        1      136      1      0.0   \n",
       "297   59    1   0       164   176    1        0       90      0      1.0   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "5        1   0     1       1  \n",
       "6        1   0     2       1  \n",
       "7        2   0     3       1  \n",
       "8        2   0     3       1  \n",
       "9        2   0     2       1  \n",
       "10       2   0     2       1  \n",
       "11       2   0     2       1  \n",
       "12       2   0     2       1  \n",
       "13       1   0     2       1  \n",
       "14       2   0     2       1  \n",
       "15       1   0     2       1  \n",
       "16       2   0     2       1  \n",
       "17       0   0     2       1  \n",
       "18       2   0     2       1  \n",
       "19       2   2     2       1  \n",
       "20       1   0     3       1  \n",
       "21       2   0     2       1  \n",
       "22       2   0     2       1  \n",
       "23       1   0     2       1  \n",
       "24       2   0     3       1  \n",
       "25       2   2     2       1  \n",
       "26       2   0     2       1  \n",
       "27       2   0     2       1  \n",
       "28       2   1     2       1  \n",
       "29       0   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "273      2   1     3       0  \n",
       "274      1   1     2       0  \n",
       "275      2   2     3       0  \n",
       "276      1   1     3       0  \n",
       "277      2   0     3       0  \n",
       "278      2   2     2       0  \n",
       "279      1   1     2       0  \n",
       "280      1   0     1       0  \n",
       "281      1   0     0       0  \n",
       "282      1   1     1       0  \n",
       "283      2   0     3       0  \n",
       "284      2   1     3       0  \n",
       "285      1   2     3       0  \n",
       "286      2   2     2       0  \n",
       "287      2   1     2       0  \n",
       "288      1   1     3       0  \n",
       "289      1   1     3       0  \n",
       "290      2   1     3       0  \n",
       "291      0   3     1       0  \n",
       "292      1   2     1       0  \n",
       "293      1   0     3       0  \n",
       "294      0   0     1       0  \n",
       "295      2   2     3       0  \n",
       "296      1   0     2       0  \n",
       "297      1   2     1       0  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "source_file = 'heart.csv'\n",
    "data = pd.read_csv(source_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last column, `target`, indicates if the patient had a heart disease. That column is used as the label and the rest are features in a 13-dimensional feature space. Separate the labels column from the rest of the dataset. (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13) (303,)\n"
     ]
    }
   ],
   "source": [
    "# Your solution\n",
    "labels = data[\"target\"]\n",
    "features = data.drop(columns=[\"target\"])\n",
    "print(features.shape, labels.shape)  # Expected: (303, 13) (303,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best classifiers\n",
    "\n",
    "We want to compare the average performance of the algorithms. To make the comparison fair they need to be evaluated on the same training and test data, in other words we need to control the randomness in training/test data split. So we use a set of *fixed random seeds* for test and train splitting. (15 points)\n",
    "\n",
    "1. Using `sklearn` split the data in two chunks, 1/4 and 3/4, looping over the predefined random seeds(from 5000 to 5051).\n",
    "2. In each iteration create a `LogisticRegression` (use `liblinear` solver) and a `KNeighborsClassifier` (k=10) then train them on the training data.\n",
    "3. In each iteration, evaluate the classifiers on the data and record their scores in the `scores` dictionary.\n",
    "4. Convert the dictionary into a pandas dataframe and call `boxplot` on the dataframe to draw a boxplot of the data. Set the y axis range to (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a16d120b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Your solution\n",
    "scores = {\n",
    "    \"KNeighborsClassifier\": [],\n",
    "    \"LogisticRegression\": []\n",
    "}\n",
    "for random_state in range(5000, 5051):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.25, random_state=random_state)\n",
    "    \n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    scores[\"LogisticRegression\"].append(lr.score(X_test, y_test))\n",
    "\n",
    "    knc = KNeighborsClassifier(n_neighbors=10)\n",
    "    knc.fit(X_train, y_train)\n",
    "    scores[\"KNeighborsClassifier\"].append(knc.score(X_test, y_test))\n",
    "    \n",
    "scores_pd = pd.DataFrame.from_dict(scores)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0,1])\n",
    "scores_pd.boxplot(ax=ax, column=[\"KNeighborsClassifier\", \"LogisticRegression\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the current settings, logistic regression outperforms KNN algorithm. Let's see if we can help KNN to get better. We'll keep the random seed as a fixed number (5026) and experiment with different values for `k`. \n",
    "\n",
    "### Improving KNN\n",
    "(20 points)\n",
    "\n",
    "1. Split the data into training and testing chunks.\n",
    "2. Train a logistic regression classifier on the training data, then evaluate it on the test data and store the value in `lr_score` variable. This is only used for comparison.\n",
    "3. Complete the test_knn function below:\n",
    "\n",
    "    1. Use a for loop to iterate through `k_range`, use these values for parameter `k` and train a KNN classifier. Pass all `knn_params` as keyword arguments to `KNeighboursClassifier`.\n",
    "    2. Create a 3-tuple (k, train_score, test_score) and append it to scores list where train_score and test_score are the results of evaluating the classifier on the training and test data. \n",
    "    3. Convert the list of tuples into a pandas dataframe. Set appropriate column names, use `k` as the index.\n",
    "\n",
    "4. Call test_knn and store the results in a variable. Use odd numbers from 3 to 25 as the first parameter.\n",
    "5. Plot the accuracy scores. Add a horizontal dashed line to show the value of `lr_score` on the same plot. It would be nicer if you used another color for this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_score  test_score\n",
      "k                          \n",
      "3      0.748899    0.657895\n",
      "5      0.748899    0.723684\n",
      "7      0.682819    0.710526\n",
      "9      0.678414    0.684211\n",
      "11     0.700441    0.710526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a179454e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 5026\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.25, random_state=seed)\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "lr_score = lr.score(X_test, y_test)\n",
    "\n",
    "def test_knn(k_range, X_train, X_test, y_train, y_test, **knn_params):\n",
    "    scores = {\n",
    "        \"k\": [],\n",
    "        \"train_score\": [],\n",
    "        \"test_score\": []\n",
    "    }\n",
    "    for k in k_range:\n",
    "        knc = KNeighborsClassifier(n_neighbors=k, **knn_params)\n",
    "        knc.fit(X_train, y_train)\n",
    "        scores[\"k\"].append(k)\n",
    "        scores[\"train_score\"].append(knc.score(X_train, y_train))\n",
    "        scores[\"test_score\"].append(knc.score(X_test, y_test))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(scores) \n",
    "    return df.set_index(\"k\")\n",
    "\n",
    "knn_score_df = test_knn(range(3,26, 2), X_train, X_test, y_train, y_test)\n",
    "print(knn_score_df.head())\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0,1])\n",
    "ax.axhline(y=lr_score,linewidth=4, color='r')\n",
    "# knn_score_df.reset_index().plot.bar(ax=ax, x=\"k\", y=\"test_score\")\n",
    "knn_score_df.plot.bar(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that with different values for K we can find the sweet spot between overfitting and generalization in KNN but it never gets good enough to beat logistic regression.\n",
    "\n",
    "We can change also experiment with changing the [`weights` parameter](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) in KNN from its default `uniform` value to `distance`. Call the `test_knn` function again with the same range for `k`, this time passing `weights` parameter as well. Plot the training and test scores as well as the horizontal dashed line that shows `lr_score`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_score  test_score\n",
      "k                          \n",
      "3           1.0    0.657895\n",
      "5           1.0    0.723684\n",
      "7           1.0    0.723684\n",
      "9           1.0    0.697368\n",
      "11          1.0    0.684211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1786d7f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAENCAYAAAD0eSVZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGXZJREFUeJzt3Xt4VdWZx/HvSwwG5CKFFK0RsRYVJEEhoD5OEapCgIqArcPF6cjY4oxKtRcGvDzgSGe00BlbRoplLE87tNy0VWmJwjAWsR2QWwFBcAjoSEBHoJARAYXknT/OgR7COZyTZB+SLH6f58nD2Xuv7HflJPyyss7e65i7IyIiYWlS3x0QEZHoKdxFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAKUNtzNbJaZfWhmm1IcNzObZmZlZrbRzLpH300REamJTEbuPwNKTnN8ANAp/jEGmFH3bomISF2kDXd3Xw786TRNbgP+3WNWAueb2YVRdVBERGouijn3i4CdCdvl8X0iIlJPzongHJZkX9I1DcxsDLGpG84777weV155ZdITvrmrImWxwota17yHaaie6jXEWqqnesmsXbt2r7vnp2sXRbiXAxcnbBcAu5M1dPeZwEyA4uJiX7NmTdITdpywKGWxNU8Oqm0/U1I91WuItVRP9ZIxs//JpF0U0zILga/Fr5q5Dqhw9/cjOK+IiNRS2pG7mc0F+gDtzKwcmATkArj7M0ApMBAoAw4Bo7PVWRERyUzacHf3EWmOO3BfZD0SEZE6i2LOXUTOIkePHqW8vJwjR47U+Vz/Njj1VdNbtmyp8/kbc728vDwKCgrIzc2tVW2Fu4jUSHl5OS1btqRjx46YJbtYLnNHyw+kPNa54Pw6nbsx13N39u3bR3l5OZdeemmtamttGRGpkSNHjtC2bds6B7ukZma0bdu2Tn8dKdxFpMYU7NlX1+dY4S4iEiDNuYtInZzuRp3aWHj/Dac9fuDAAebMmcO9995bo/MOHDiQh6fOoFXr6O9EbYg0cheRRuXAgQP8+Mc/PmV/ZWXlaT+vtLS0QQR7un5GReEuIo3KhAkT2L59O1dffTU9e/akb9++jBw5ksLCQgCGDBlCjx49uOqqq5g5c+aJz+vYsSP7/7SPXTvfY0jfa/mHv3+AoTddzz0jh3Hk8OGU9aZNm0aXLl0oKipi+PDhABw8eJDRo0dTWFhIUVERv/rVrwCYO3cuhYWFdO3alfHjx584x3VXFDD9B//EqFtvZsPaVby1cT033ngjPXr0oH///rz/fvQ39WtaRkQalSeffJJNmzaxfv16li1bxqBBg9i0adOJSwZnzZrFZz7zGQ4fPkzPnj25/fbbadu27UnneO+d7Tz59LNMmvIjxv3daJa+vJBene5JWe+dd97h3HPP5cCB2KWNkydPpnXr1rz55psA7N+/n927dzN+/HjWrl1LmzZt6NevHxe+sogvlQzi8KGP+cIVnbnvuw9z9OhR7v7qIP7j5UXk5+czf/58HnnkEWbNmhXp86RwF5FGrVevXiddCz5t2jReeOEFAHbu3Mm2bdtOCfeLLr6EK6+KjfQ7F3Zj986dpFJUVMSoUaMYMmQIQ4YMAWDp0qXMmzfvRJs2bdqwfPly+vTpQ35+bMHGUaNGsWzlf/GlkkHk5ORw88DBALy7fRtlb2/llltuAWLTNBdeGP1bYCjcRaRRO++88048XrZsGUuXLmXFihU0b96cPn36JL1WPLdp0xOPc5rk8Ell6uvJFy1axPLly1m4cCGTJ09m8+bNuPsplyrGVmJJrum5eeTk5BxvyGWXX8n6tasy/RJrRXPuItKotGzZko8++ijpsYqKCtq0aUPz5s3ZunUrK1eurFOtqqoqdu7cSd++fZkyZQoHDhzg4MGD9OvXj6effvpEu/3793Pttdfy2muvsXfvXiorK5k7dy7F15165U/Hyzqxf99eVqxYAcSWc9i8eXOd+pmMRu4iUifv1mEd9I2nuT0/lbZt23LDDTfQtWtXmjVrRvv27U8cKykp4ZlnnqGoqIgrrriC6667rtZ9g9iUyZ133klFRQXuzre+9S3OP/98Hn30Ue677z66du1KTk4OkyZNYtiwYTzxxBP07dsXd2fgwIH07T/wlHPmNm3KD37yc8aPH09FRQXHjh3jwQcf5KqrrqpTX6uz0/0pkU3FZp78rTpEpCHb8vLLdG7Xrr67cVbYsncvnQcMOGmfwVp3L073uZqWEREJkKZlRESA+77/ff6wYcNJ+x4YPpzRgwfXU4/qRuEuIgJMT7jpKAT1F+49ekAt3iC7Li/epKJ6qtcQazXYelu2QOfOkdQ73QuqRVlYX73R1duyBaq/LprhapGacxcRCZDCXUQkQAp3EWlUUq0KmYlfPDuDw4cPRdyjhkkvqIpI3TxW+2V0i5Ls2/j1/znt5xwP95qu5w7wy5/OYNCwO2jWrHmNP7c2Kisr/7zswBmmkbuINCqJS/6OGzeOqVOn0rNnT4qKipg0aRIAH3/8MYMGDaJbt2507dqV+fPnM23aND783w/4+h23cvcdtyY9d2VlJXfddRddu3alsLCQp556CoCysjJuvvlmunXrRvfu3dm+fTvuzrhx4060nT9/PhBb3+b4MsRfuSW2/MBvfz2fkV++iTv6f5HHJzx4RtZ018hdRBqVxCV/lyxZwvPPP8+qVatwdwYPHszy5cvZs2cPn/vc51i0KHb1T0VFBa1bt+b7U3/Aswt+Q5vPtE167vXr17Nr1y42bdoEcGKJ31GjRjFhwgSGDh3KkSNHqKqq4te//jXr169nw4YN7N27l549e9K7d28AVq1axaZNm/gotw07tr3N4t+8wM9feIXc3Fz+8eHvUPrCc1zz4N9m9XnSyF1EGq0lS5awZMkSrrnmGrp3787WrVvZtm0bhYWFLF26lPHjx/P666/TOsN3YPr85z/Pjh07GDt2LK+88gqtWrXio48+YteuXQwdOhSAvLw8mjdvzu9//3tGjBhBTk4O7du358Ybb2T16tXAycsQv/GH19iycQOjvvwl7uj/Rd74w3LK33s3K89HIo3cRaTRcnceeugh7rnn1DfaWLt2LaWlpTz00EP069ePiRMnpj1fmzZt2LBhA4sXL2b69OksWLCAH/7whylrp5K4DLE73PrV4TwwYVIGX1F0NHIXkUYlccnf/v37M2vWLA4ePAjArl27+PDDD9m9ezfNmzfnzjvv5Lvf/S7r1q0DoPl5Lfg43jaZvXv3UlVVxe23387kyZNZt24drVq1oqCggBdffBGATz75hEOHDtG7d2/mz59PZWUle/bsYfny5fTq1euUc157Q2+WLlrIvr17AKjYv5/d5e9F+pwko5G7iDQqiUv+DhgwgJEjR3L99dcD0KJFC37xi19QVlbGuHHjaNKkCbm5ucyYMQOA20fdxX1f+yrtPtueny74zSnn3rVrF6NHj6aqqgqAJ554AoDZs2dzzz33MHHiRHJzc3nuuecYOnQoK1asoFu3bpgZU6ZM4YILLmDr1q0nnfOyy6/kvnGP8HejhlFVVcU5ubk8/L2p2XyKAIW7iNTVYxW1/tTarOcOMGfOnJO2H3jggZO2L7vsMvr373/K540cPYaRo8ekPG+3bt1OjPITderUiVdfffWU/VOnTmXq1JODuk+fPvTp0+ekfSWDh1EyeFjKutmgaRkRkQBp5C4iZ6VRt97M0U8/ObGdl5vD7NmzKSwsrMdeRUfhLiJnpV/+ZulJ29lYFbI+aVpGRGqsvt6e82xS1+dY4S4iNZKXl8e+ffsU8Fnk7uzbt4+8vLxan0PTMiJSIwUFBZSXl7Nnz546n+t/9x9OeWzLR83qfP7GXC8vL4+CgoJa184o3M2sBPgRkAM86+5PVjveAfg5cH68zQR3L611r0SkwcrNzT1xa31dDTjD7zQVer1EaadlzCwHmA4MALoAI8ysS7VmjwIL3P0aYDhQu8WWRUQkEpnMufcCytx9h7t/CswDbqvWxoFW8cetgd3RdVFERGoqk3C/CNiZsF0e35foMeBOMysHSoGxyU5kZmPMbI2ZrYlivk5ERJLLJNyTvdV29ZfJRwA/c/cCYCAw28xOObe7z3T3Yncvzs/Pr3lvRUQkI5m8oFoOXJywXcCp0y53AyUA7r7CzPKAdsCHUXRSRM5Cp3v7vjqsZ3O2yGTkvhroZGaXmllTYi+YLqzW5j3gJgAz6wzkAZp3ERGpJ2nD3d2PAfcDi4EtxK6K2Wxmj5vZ4Hiz7wDfMLMNwFzgLtcdDiIi9Saj69zj16yXVts3MeHxW8AN0XZNRBoUTZM0Klp+QEQkQAp3EZEAaW0ZEREIbtpJI3cRkQAp3EVEAqRpGZHGKrBpBImWRu4iIgFSuIuIBEjTMhK2Mzl1oWkSqYks/7xo5C4iEiCFu4hIgDQtk86Z/lNbf9qLSAQ0chcRCZDCXUQkQAp3EZEAKdxFRAKkF1TPdnoBVyRIGrmLiARI4S4iEiBNy8iZpWkgkTNCI3cRkQAp3EVEAqRwFxEJkMJdRCRACncRkQA1vqtldLWFiEhaGrmLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIByijczazEzN42szIzm5CizR1m9paZbTazOdF2U0REaiLtHapmlgNMB24ByoHVZrbQ3d9KaNMJeAi4wd33m9lns9VhERFJL5ORey+gzN13uPunwDzgtmptvgFMd/f9AO7+YbTdFBGRmsgk3C8CdiZsl8f3JbocuNzM/mBmK82sJKoOiohIzWWycJgl2edJztMJ6AMUAK+bWVd3P3DSiczGAGMAOnToUOPOiohIZjIZuZcDFydsFwC7k7R5yd2Puvs7wNvEwv4k7j7T3YvdvTg/P7+2fRYRkTQyCffVQCczu9TMmgLDgYXV2rwI9AUws3bEpml2RNlRERHJXNpwd/djwP3AYmALsMDdN5vZ42Y2ON5sMbDPzN4CfgeMc/d92eq0iIicXkZv1uHupUBptX0TEx478O34h4iI1DPdoSoiEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIByijczazEzN42szIzm3Cadl8xMzez4ui6KCIiNZU23M0sB5gODAC6ACPMrEuSdi2BbwJvRN1JERGpmUxG7r2AMnff4e6fAvOA25K0mwxMAY5E2D8REamFTML9ImBnwnZ5fN8JZnYNcLG7//Z0JzKzMWa2xszW7Nmzp8adFRGRzGQS7pZkn584aNYEeAr4TroTuftMdy929+L8/PzMeykiIjWSSbiXAxcnbBcAuxO2WwJdgWVm9i5wHbBQL6qKiNSfTMJ9NdDJzC41s6bAcGDh8YPuXuHu7dy9o7t3BFYCg919TVZ6LCIiaaUNd3c/BtwPLAa2AAvcfbOZPW5mg7PdQRERqblzMmnk7qVAabV9E1O07VP3bomISF3oDlURkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEAqRwFxEJkMJdRCRACncRkQAp3EVEApRRuJtZiZm9bWZlZjYhyfFvm9lbZrbRzP7TzC6JvqsiIpKptOFuZjnAdGAA0AUYYWZdqjX7I1Ds7kXA88CUqDsqIiKZy2Tk3gsoc/cd7v4pMA+4LbGBu//O3Q/FN1cCBdF2U0REaiKTcL8I2JmwXR7fl8rdwMt16ZSIiNTNORm0sST7PGlDszuBYuDGFMfHAGMAOnTokGEXRUSkpjIZuZcDFydsFwC7qzcys5uBR4DB7v5JshO5+0x3L3b34vz8/Nr0V0REMpBJuK8GOpnZpWbWFBgOLExsYGbXAD8hFuwfRt9NERGpibTh7u7HgPuBxcAWYIG7bzazx81scLzZVKAF8JyZrTezhSlOJyIiZ0Amc+64eylQWm3fxITHN0fcLxERqQPdoSoiEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIBUriLiARI4S4iEiCFu4hIgBTuIiIByijczazEzN42szIzm5Dk+LlmNj9+/A0z6xh1R0VEJHNpw93McoDpwACgCzDCzLpUa3Y3sN/dvwA8BXw/6o6KiEjmMhm59wLK3H2Hu38KzANuq9bmNuDn8cfPAzeZmUXXTRERqQlz99M3MPsKUOLuX49v/xVwrbvfn9BmU7xNeXx7e7zN3mrnGgOMiW9eAbxdiz63A/ambRUd1VO9hlhL9c7eepe4e366RudkcKJkI/DqvxEyaYO7zwRmZlAzdWfM1rh7cV3OoXqq19hrqZ7qpZPJtEw5cHHCdgGwO1UbMzsHaA38KYoOiohIzWUS7quBTmZ2qZk1BYYDC6u1WQj8dfzxV4BXPd18j4iIZE3aaRl3P2Zm9wOLgRxglrtvNrPHgTXuvhD4KTDbzMqIjdiHZ7HPdZrWUT3VC6SW6qneaaV9QVVERBof3aEqIhIghbuISIAU7iIiAcrkOvd6ZWa9AHf31fFlD0qAre5eegZq/wWxO3Q3ufuSLJz/m8AL7r4z6nOnqHcZMJTYZavHgG3AXHevOBP1ReTMadAjdzObBEwDZpjZE8DTQAtggpk9koV6qxIefyNeryUwKdmCaRGYDLxhZq+b2b1mlvaus9qK/yJ5BsgDegLNiIX8CjPrk626InVlZi9n4ZwXmNkMM5tuZm3N7DEze9PMFpjZhVmoV5LwuLWZ/dTMNprZHDNrH3U9ANy9wX4AbxK7/LI58H9Aq/j+ZsDGLNT7Y8Lj1UB+/PF5wJvZqEfsF2w/YpeT7gFeIXbPQMtsPJfxx82BZfHHHRK/7jP4vX05C+e8AJhBbKG7tsBj8a97AXBhFuqVJDxuHf8ebgTmAO0jrrUOeBS47Ax9f1oAjwObgYr4z+ZK4K4s1eue4qMH8H4W6r0CjAUmxL9n4+P/F8YCL2Wh3rqEx88C3wMuAb4FvJiN57ShT8scc/dK4JCZbXf3/wNw98NmVpWFek3MrA2xwDV33xOv97GZHctCPXf3KmAJsMTMcomtvjkC+AEQ9Uj+HKASOJfYXyS4+3vxupEzs+6pDgFXZ6Hkz4BFxH4Z/w74JTCI2MJ2z3Dqgnd19U/EQgLgn4H3gVuBYcBPgCER1moDnA/8zsw+AOYC8929+t3iUfkl8ALQH7iD2HM6D3jUzC5394cjrrcaeI3kS5mcH3EtiP3y/VcAM7vX3Y+vZPuvZnZ3FuolKnb34z//T5nZX5+2dW2diVFAHX7bvQE0jz9ukrC/NQm/CSOs9y6wA3gn/u8F8f0tgPVZqJdyxAw0i7jWA8RGKDOBrcDo+P58YHmWvn+VwKvEgrb6x+FsPp/Ae9WOZeP7ty7V+aOuV63WF4EfAx/En8sxWfjaNlTbXh3/twmx17yirrcJ6JTi2M5sfn3A96ody8Zf6eXAt4HvxLPFEo5FPgvh3vBH7r3d/RMAj41wj8vlz8sdRMbdO6Y4VEXshcio/eVp+nI4ykLu/iMzWwp0Bv7F3bfG9+8BekdZK8EW4B5331b9gJll40XkxNeQ/r3asZws1PusmX2b2GizlZmZx/+3ksXXs9z9deB1MxsL3ELs5yjqux0/NrO/cPffm9mtxNeKcveqLC3n/Ripn7OxWaj3kpm1cPeD7v7o8Z1m9gVqt1ptOv9G/K9lYsujtwP2mNkFwPos1GvY4X482JPs38sZXJrT3Q8RG81Hfd7/jvqcaeptJjaHeqY8hv7DRuWUnxWPTVm+wp+nhqL0t8CzZnY5sVH13wDEX/SfHnUxd3/ezK40s5uAN9z9YMLhI1moNzFe76LEeu5eZmbPZqHeP5jZlUD1eh+Y2Zyo6x0vqg99nPEP4tNCqte4amWrHvBNYr+AXyQ2PXpbwrFsTMGODbmeu2ttGakfZvaeu3dQvcZVK1v1zOxN4Hp3Pxh/D+bngdkem078o7tfo3o106CnZaRxM7ONqQ4BkV/bG3K9kL+2uBz/81TFu/F7L543s0tIfgWN6qWhcJdsak/sUrr91fYb8F+q12Br1Ue9D8zsandfDxAf4X4ZmAUUql7NKdwlm34LtDj+A53IzJapXoOtVR/1vkZsSYwT3P0Y8DUz+4nq1Zzm3EVEAtSg15YREZHaUbiLiARI4S4SZ2YdzWxTffdDJAoKdxGRACncRZIws8+b2R/NrGd990WkNhTuItWY2RXAr4jdZr+6vvsjUhu6zl3kZPnAS8DtHltoTaRR0shd5GQVwE7ghvruiEhdaOQucrJPib2D0mIzO+ju2VmOVSTLFO4i1XjsbRW/DPyHmX3s7i/Vd59EakrLD4iIBEhz7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4i4gESOEuIhIghbuISID+Hwd/t3g16yElAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your solution\n",
    "knn_score_df = test_knn(range(3,26, 2), X_train, X_test, y_train, y_test, weights=\"distance\")\n",
    "print(knn_score_df.head())\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0,1])\n",
    "ax.axhline(y=lr_score,linewidth=4, color='r')\n",
    "# knn_score_df.reset_index().plot.bar(ax=ax, x=\"k\", y=\"test_score\")\n",
    "knn_score_df.plot.bar(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it's overfitting, it performs better on the test set than the previous one. Yet, it's no better than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the models (ensemble model)\n",
    "\n",
    "A common way of maximizing the accuracy is to train multiple models and use all for prediction. In a classification problem we use the class that the majority of models have predicted and in a regression problem a weighted average of the predictions is used.\n",
    "\n",
    "Make a function that creates and trains 3 models and returns them in a list. The models are:\n",
    "\n",
    "* A Logistic Regressor with 'liblinear' solver\n",
    "* A KNN classifiers with k = 10 and setting `weights` to 'distance'\n",
    "* Another KNN with k = 20 and setting `weights` to 'distance'\n",
    "\n",
    "and the inputs to the function are X and Y values for the training set. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(X_train, y_train):\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    knc10 = KNeighborsClassifier(n_neighbors=10, weights=\"distance\")\n",
    "    knc10.fit(X_train, y_train)\n",
    "    knc20 = KNeighborsClassifier(n_neighbors=20, weights=\"distance\")\n",
    "    knc20.fit(X_train, y_train)\n",
    "    return [lr, knc10, knc20]\n",
    "\n",
    "models = create_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `predict` function that takes in a list of models and the input vectors (X) and returns the output vector Y using majority voting. To find the majority in an easy way use `mode` function in `scipy` since it supports vectorized operations. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def predict(models, X):\n",
    "    return mode([m.predict(X) for m in models])[0].reshape(-1)\n",
    "    \n",
    "predict(models, X_test)  # Expected: 0, 1, 1, 1, 1, 0, 0, 0, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a loop similar to the first part (Finding the best classifiers) to compare the performance of each of the models in the ensemble (=the model list created by `create_models`) and the combined model (call the `predict` function above). To evaluate the accuracy of the combined model use `sklearn.metrics.accuracy_score` method. Draw the boxplot in the end. (13 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a17700198>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Your solution\n",
    "# Use LR, KNN15, KNN20, and Combined for the column names\n",
    "scores = {\n",
    "    \"LR\": [],\n",
    "    \"KNN10\": [],\n",
    "    \"KNN20\": [],\n",
    "    \"Combined\": []\n",
    "}\n",
    "for random_state in range(5000, 5051):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.25, random_state=random_state)\n",
    "    \n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    scores[\"LR\"].append(lr.score(X_test, y_test))\n",
    "\n",
    "    knc10 = KNeighborsClassifier(n_neighbors=10)\n",
    "    knc10.fit(X_train, y_train)\n",
    "    scores[\"KNN10\"].append(knc10.score(X_test, y_test))\n",
    "    \n",
    "    knc20 = KNeighborsClassifier(n_neighbors=20)\n",
    "    knc20.fit(X_train, y_train)\n",
    "    scores[\"KNN20\"].append(knc20.score(X_test, y_test))\n",
    "    \n",
    "    scores[\"Combined\"].append(accuracy_score(y_test, predict([lr, knc10, knc20], X_test)))\n",
    "\n",
    "scores_pd = pd.DataFrame.from_dict(scores)\n",
    "scores_pd\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0,1])\n",
    "scores_pd.boxplot(ax=ax, column=[\"LR\", \"KNN10\", \"KNN20\", \"Combined\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the result get better than the individual KNN models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem2: Bike sharing (25 points)\n",
    "\n",
    "In this problem we are going to use linear regression to predict the number of bike sharif platfor quality of wine from 1-10. Read the data from CSV file, use 'cnt' column as the target values and all other columns (except 'dteday') as the features (4 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         dteday  season  yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
      "0    2011-01-01       1   0     1        0        6           0           2   \n",
      "1    2011-01-02       1   0     1        0        0           0           2   \n",
      "2    2011-01-03       1   0     1        0        1           1           1   \n",
      "3    2011-01-04       1   0     1        0        2           1           1   \n",
      "4    2011-01-05       1   0     1        0        3           1           1   \n",
      "5    2011-01-06       1   0     1        0        4           1           1   \n",
      "6    2011-01-07       1   0     1        0        5           1           2   \n",
      "7    2011-01-08       1   0     1        0        6           0           2   \n",
      "8    2011-01-09       1   0     1        0        0           0           1   \n",
      "9    2011-01-10       1   0     1        0        1           1           1   \n",
      "10   2011-01-11       1   0     1        0        2           1           2   \n",
      "11   2011-01-12       1   0     1        0        3           1           1   \n",
      "12   2011-01-13       1   0     1        0        4           1           1   \n",
      "13   2011-01-14       1   0     1        0        5           1           1   \n",
      "14   2011-01-15       1   0     1        0        6           0           2   \n",
      "15   2011-01-16       1   0     1        0        0           0           1   \n",
      "16   2011-01-17       1   0     1        1        1           0           2   \n",
      "17   2011-01-18       1   0     1        0        2           1           2   \n",
      "18   2011-01-19       1   0     1        0        3           1           2   \n",
      "19   2011-01-20       1   0     1        0        4           1           2   \n",
      "20   2011-01-21       1   0     1        0        5           1           1   \n",
      "21   2011-01-22       1   0     1        0        6           0           1   \n",
      "22   2011-01-23       1   0     1        0        0           0           1   \n",
      "23   2011-01-24       1   0     1        0        1           1           1   \n",
      "24   2011-01-25       1   0     1        0        2           1           2   \n",
      "25   2011-01-26       1   0     1        0        3           1           3   \n",
      "26   2011-01-27       1   0     1        0        4           1           1   \n",
      "27   2011-01-28       1   0     1        0        5           1           2   \n",
      "28   2011-01-29       1   0     1        0        6           0           1   \n",
      "29   2011-01-30       1   0     1        0        0           0           1   \n",
      "..          ...     ...  ..   ...      ...      ...         ...         ...   \n",
      "701  2012-12-02       4   1    12        0        0           0           2   \n",
      "702  2012-12-03       4   1    12        0        1           1           1   \n",
      "703  2012-12-04       4   1    12        0        2           1           1   \n",
      "704  2012-12-05       4   1    12        0        3           1           1   \n",
      "705  2012-12-06       4   1    12        0        4           1           1   \n",
      "706  2012-12-07       4   1    12        0        5           1           2   \n",
      "707  2012-12-08       4   1    12        0        6           0           2   \n",
      "708  2012-12-09       4   1    12        0        0           0           2   \n",
      "709  2012-12-10       4   1    12        0        1           1           2   \n",
      "710  2012-12-11       4   1    12        0        2           1           2   \n",
      "711  2012-12-12       4   1    12        0        3           1           2   \n",
      "712  2012-12-13       4   1    12        0        4           1           1   \n",
      "713  2012-12-14       4   1    12        0        5           1           1   \n",
      "714  2012-12-15       4   1    12        0        6           0           1   \n",
      "715  2012-12-16       4   1    12        0        0           0           2   \n",
      "716  2012-12-17       4   1    12        0        1           1           2   \n",
      "717  2012-12-18       4   1    12        0        2           1           1   \n",
      "718  2012-12-19       4   1    12        0        3           1           1   \n",
      "719  2012-12-20       4   1    12        0        4           1           2   \n",
      "720  2012-12-21       1   1    12        0        5           1           2   \n",
      "721  2012-12-22       1   1    12        0        6           0           1   \n",
      "722  2012-12-23       1   1    12        0        0           0           1   \n",
      "723  2012-12-24       1   1    12        0        1           1           2   \n",
      "724  2012-12-25       1   1    12        1        2           0           2   \n",
      "725  2012-12-26       1   1    12        0        3           1           3   \n",
      "726  2012-12-27       1   1    12        0        4           1           2   \n",
      "727  2012-12-28       1   1    12        0        5           1           2   \n",
      "728  2012-12-29       1   1    12        0        6           0           2   \n",
      "729  2012-12-30       1   1    12        0        0           0           1   \n",
      "730  2012-12-31       1   1    12        0        1           1           2   \n",
      "\n",
      "         temp     atemp       hum  windspeed   cnt  \n",
      "0    0.344167  0.363625  0.805833   0.160446   985  \n",
      "1    0.363478  0.353739  0.696087   0.248539   801  \n",
      "2    0.196364  0.189405  0.437273   0.248309  1349  \n",
      "3    0.200000  0.212122  0.590435   0.160296  1562  \n",
      "4    0.226957  0.229270  0.436957   0.186900  1600  \n",
      "5    0.204348  0.233209  0.518261   0.089565  1606  \n",
      "6    0.196522  0.208839  0.498696   0.168726  1510  \n",
      "7    0.165000  0.162254  0.535833   0.266804   959  \n",
      "8    0.138333  0.116175  0.434167   0.361950   822  \n",
      "9    0.150833  0.150888  0.482917   0.223267  1321  \n",
      "10   0.169091  0.191464  0.686364   0.122132  1263  \n",
      "11   0.172727  0.160473  0.599545   0.304627  1162  \n",
      "12   0.165000  0.150883  0.470417   0.301000  1406  \n",
      "13   0.160870  0.188413  0.537826   0.126548  1421  \n",
      "14   0.233333  0.248112  0.498750   0.157963  1248  \n",
      "15   0.231667  0.234217  0.483750   0.188433  1204  \n",
      "16   0.175833  0.176771  0.537500   0.194017  1000  \n",
      "17   0.216667  0.232333  0.861667   0.146775   683  \n",
      "18   0.292174  0.298422  0.741739   0.208317  1650  \n",
      "19   0.261667  0.255050  0.538333   0.195904  1927  \n",
      "20   0.177500  0.157833  0.457083   0.353242  1543  \n",
      "21   0.059130  0.079070  0.400000   0.171970   981  \n",
      "22   0.096522  0.098839  0.436522   0.246600   986  \n",
      "23   0.097391  0.117930  0.491739   0.158330  1416  \n",
      "24   0.223478  0.234526  0.616957   0.129796  1985  \n",
      "25   0.217500  0.203600  0.862500   0.293850   506  \n",
      "26   0.195000  0.219700  0.687500   0.113837   431  \n",
      "27   0.203478  0.223317  0.793043   0.123300  1167  \n",
      "28   0.196522  0.212126  0.651739   0.145365  1098  \n",
      "29   0.216522  0.250322  0.722174   0.073983  1096  \n",
      "..        ...       ...       ...        ...   ...  \n",
      "701  0.347500  0.359208  0.823333   0.124379  4649  \n",
      "702  0.452500  0.455796  0.767500   0.082721  6234  \n",
      "703  0.475833  0.469054  0.733750   0.174129  6606  \n",
      "704  0.438333  0.428012  0.485000   0.324021  5729  \n",
      "705  0.255833  0.258204  0.508750   0.174754  5375  \n",
      "706  0.320833  0.321958  0.764167   0.130600  5008  \n",
      "707  0.381667  0.389508  0.911250   0.101379  5582  \n",
      "708  0.384167  0.390146  0.905417   0.157975  3228  \n",
      "709  0.435833  0.435575  0.925000   0.190308  5170  \n",
      "710  0.353333  0.338363  0.596667   0.296037  5501  \n",
      "711  0.297500  0.297338  0.538333   0.162937  5319  \n",
      "712  0.295833  0.294188  0.485833   0.174129  5532  \n",
      "713  0.281667  0.294192  0.642917   0.131229  5611  \n",
      "714  0.324167  0.338383  0.650417   0.106350  5047  \n",
      "715  0.362500  0.369938  0.838750   0.100742  3786  \n",
      "716  0.393333  0.401500  0.907083   0.098258  4585  \n",
      "717  0.410833  0.409708  0.666250   0.221404  5557  \n",
      "718  0.332500  0.342162  0.625417   0.184092  5267  \n",
      "719  0.330000  0.335217  0.667917   0.132463  4128  \n",
      "720  0.326667  0.301767  0.556667   0.374383  3623  \n",
      "721  0.265833  0.236113  0.441250   0.407346  1749  \n",
      "722  0.245833  0.259471  0.515417   0.133083  1787  \n",
      "723  0.231304  0.258900  0.791304   0.077230   920  \n",
      "724  0.291304  0.294465  0.734783   0.168726  1013  \n",
      "725  0.243333  0.220333  0.823333   0.316546   441  \n",
      "726  0.254167  0.226642  0.652917   0.350133  2114  \n",
      "727  0.253333  0.255046  0.590000   0.155471  3095  \n",
      "728  0.253333  0.242400  0.752917   0.124383  1341  \n",
      "729  0.255833  0.231700  0.483333   0.350754  1796  \n",
      "730  0.215833  0.223487  0.577500   0.154846  2729  \n",
      "\n",
      "[731 rows x 13 columns]\n",
      "(731, 11) (731,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "\n",
    "# Your solution\n",
    "source_file = 'bikes.csv'\n",
    "data = pd.read_csv(source_file)\n",
    "print(data)\n",
    "target = data[\"cnt\"]\n",
    "features = data.drop(columns=[\"cnt\", \"dteday\"])\n",
    "print(features.shape, target.shape) # Expected: (731, 11) (731,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data in two 1/4 and 3/4 chunks for training and testing, use the number 15 as the `random_state` parameter. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear regression model on the data and report the training and test scores. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training=0.7989\tTest=0.7857\n"
     ]
    }
   ],
   "source": [
    "# Your solution\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "training_score = lr.score(X_train, y_train)\n",
    "test_score = lr.score(X_test, y_test)\n",
    "print(f'Training={training_score:.4}\\tTest={test_score:.4}')  # Expected: Training=0.7857\tTest=0.7989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularize the regression model using ridge method. Train it with 10 values of alpha and report training and test scores. (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⍺=0.001\tTraining=0.7989\tTest=0.7858\n",
      "⍺=0.01\tTraining=0.7989\tTest=0.7865\n",
      "⍺=0.1\tTraining=0.7987\tTest=0.788\n",
      "⍺=1.0\tTraining=0.7975\tTest=0.7875\n",
      "⍺=2.0\tTraining=0.7956\tTest=0.7856\n",
      "⍺=5.0\tTraining=0.789\tTest=0.7781\n",
      "⍺=10.0\tTraining=0.7768\tTest=0.7638\n",
      "⍺=20.0\tTraining=0.7511\tTest=0.7339\n",
      "⍺=50.0\tTraining=0.6836\tTest=0.6562\n",
      "⍺=100.0\tTraining=0.601\tTest=0.5631\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [1e-3, 1e-2, 1e-1, 1., 2., 5., 10., 20., 50., 100.]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Your solution\n",
    "    r = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    training_score = r.score(X_train, y_train)\n",
    "    test_score = r.score(X_test, y_test)\n",
    "    print(f'⍺={alpha:.5}\\tTraining={training_score:.4}\\tTest={test_score:.4}')\n",
    "\n",
    "# First line of expected value:\n",
    "# ⍺=0.001\tTraining=0.7858\tTest=0.7989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same alpha values, train a lasso regualizer this time. Report the names of the features that got a 0 coefficient after regulazition. Separate their names with a comma, if none of them were omitted report 'no features'. (9 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⍺=0.001\tTraining=0.7989\tTest=0.7857\tno features\n",
      "⍺=0.01\tTraining=0.7989\tTest=0.7857\tno features\n",
      "⍺=0.1\tTraining=0.7989\tTest=0.7859\tno features\n",
      "⍺=1.0\tTraining=0.7988\tTest=0.7865\tno features\n",
      "⍺=2.0\tTraining=0.7984\tTest=0.7869\tno features\n",
      "⍺=5.0\tTraining=0.796\tTest=0.7857\tno features\n",
      "⍺=10.0\tTraining=0.7889\tTest=0.7783\thum\n",
      "⍺=20.0\tTraining=0.7796\tTest=0.7694\tholiday, atemp, hum, windspeed\n",
      "⍺=50.0\tTraining=0.7571\tTest=0.7476\tholiday, atemp, hum, windspeed\n",
      "⍺=100.0\tTraining=0.6854\tTest=0.6755\tmnth, holiday, workingday, atemp, hum, windspeed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [1e-3, 1e-2, 1e-1, 1., 2., 5., 10., 20., 50., 100.]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Your solution\n",
    "    l = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "    training_score = l.score(X_train, y_train)\n",
    "    test_score = l.score(X_test, y_test)\n",
    "    zeroed_out_feature_names = \", \".join([name for name, val in zip(list(X_train.columns.values), l.coef_) if val == 0])\n",
    "    zeroed_out_feature_names = zeroed_out_feature_names if zeroed_out_feature_names else \"no features\"\n",
    "    print(f'⍺={alpha:.5}\\tTraining={training_score:.4}\\tTest={test_score:.4}\\t{zeroed_out_feature_names}')\n",
    "    \n",
    "# First line of expected value:\n",
    "# ⍺=0.001\tTraining=0.7857\tTest=0.7989\tno features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
