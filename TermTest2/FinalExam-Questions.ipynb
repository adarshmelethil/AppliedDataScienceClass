{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> ENSF 519.01 Applied Data Scince </center></h1>\n",
    "<h2> <center> Term Test 2 - Nov 27, 2019 </center></h2>\n",
    "<h2> <center> 100 marks - 2 hours </center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your Full Name:` Adarsh Melethil\n",
    "\n",
    "`Your Student ID:` 10135476"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: Classification (35 pts)\n",
    "\n",
    "In this question, you will apply some classification methods on the Breast Cancer dataset, provided by SKLearn. The breast cancer dataset is a classic binary classification dataset that can be found here:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "     \n",
    "     \n",
    "\n",
    "After loading the dataset, spilt the data into test and train subsets using K-Fold(k=5) - use random_state=0. Then follow the 3 steps, explianed below:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that has not been mentioned.\n",
    "</div>\n",
    "\n",
    "* Step1:  Build 3 classification models using LinearSVC, GaussianNB, and KNeighborsClassifier from SKLearn, with default settings. Each model gets all features of the dataset as the feature set and predicts the the target as 'malignant' or 'benign'. \n",
    "\n",
    "    * Report your prediction results as three confusion matrices; one matrix per model (representing the mean of 5 folds results per model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/datascience/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/envs/datascience/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/envs/datascience/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/envs/datascience/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvc\n",
      "[[171  41]\n",
      " [ 11 346]]\n",
      "gnb\n",
      "[[188  24]\n",
      " [ 12 345]]\n",
      "knc\n",
      "[[183  29]\n",
      " [ 13 344]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/datascience/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "data_bunch = load_breast_cancer()\n",
    "\n",
    "features = pd.DataFrame(data=data_bunch.data, columns=data_bunch.feature_names)\n",
    "labels = pd.DataFrame(data=data_bunch.target, columns=[\" or \".join(data_bunch.target_names)])\n",
    "\n",
    "predictions = {\n",
    "    name: []\n",
    "    for name in [\"lsvc\", \"gnb\", \"knc\"]\n",
    "}\n",
    "actuals = []\n",
    "for indexes in kf.split(features):\n",
    "    train_index, test_index = indexes\n",
    "    train_data = (features.iloc[train_index], labels.iloc[train_index].values.ravel())\n",
    "    test_data = (features.iloc[test_index], labels.iloc[test_index].values.ravel())\n",
    "\n",
    "    predictions[\"lsvc\"].extend(LinearSVC().fit(*train_data).predict(test_data[0]))\n",
    "    predictions[\"gnb\"].extend(GaussianNB().fit(*train_data).predict(test_data[0]))\n",
    "    predictions[\"knc\"].extend(KNeighborsClassifier().fit(*train_data).predict(test_data[0]))\n",
    "    \n",
    "    actuals.extend(test_data[1])\n",
    "\n",
    "def numToName(vals):\n",
    "    return [data_bunch.target_names[v] for v in vals]\n",
    "\n",
    "for name, pred in predictions.items():\n",
    "    print(name)\n",
    "    print(confusion_matrix(numToName(actuals), numToName(pred), labels=data_bunch.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which model(s) has the highest False Postive and which one(s) has the highest False Negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsvc has the highest false positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: Normalize your data using StandardScaler. Then build the same classification models (LinearSVC, GaussianNB, and KNN) this time using the scaled data. \n",
    "\n",
    "    * Report your prediction results in the form of three confusion matrices, again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvc\n",
      "[[203   9]\n",
      " [  9 348]]\n",
      "gnb\n",
      "[[189  23]\n",
      " [ 17 340]]\n",
      "knc\n",
      "[[195  17]\n",
      " [  6 351]]\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "predictions = {\n",
    "    name: []\n",
    "    for name in [\"lsvc\", \"gnb\", \"knc\"]\n",
    "}\n",
    "actuals = []\n",
    "for indexes in kf.split(features):\n",
    "    train_index, test_index = indexes\n",
    "    \n",
    "    train_data = [features.iloc[train_index], labels.iloc[train_index].values.ravel()]\n",
    "    test_data = [features.iloc[test_index], labels.iloc[test_index].values.ravel()]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[0])\n",
    "    train_data[0] = scaler.transform(train_data[0])\n",
    "    test_data[0] = scaler.transform(test_data[0])\n",
    "    \n",
    "    predictions[\"lsvc\"].extend(LinearSVC().fit(*train_data).predict(test_data[0]))\n",
    "    predictions[\"gnb\"].extend(GaussianNB().fit(*train_data).predict(test_data[0]))\n",
    "    predictions[\"knc\"].extend(KNeighborsClassifier().fit(*train_data).predict(test_data[0]))\n",
    "    \n",
    "    actuals.extend(test_data[1])\n",
    "\n",
    "def numToName(vals):\n",
    "    return [data_bunch.target_names[v] for v in vals]\n",
    "\n",
    "for name, pred in predictions.items():\n",
    "    print(name)\n",
    "    print(confusion_matrix(numToName(actuals), numToName(pred), labels=data_bunch.target_names))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Identify which model(s) has most improved with scaling. Explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    lsvm has imporoved the most.\\n    the distribution did not change with scaling so the Gaussian and clustering would not be effected as much because the shape of the data did not change.\\n    \\n    lsvm uses the values the valuses of features in it's calculation, so with scaling it does not favour a particular feature because of the unit difference\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer\n",
    "\"\"\"\n",
    "    lsvm has imporoved the most.\n",
    "    the distribution did not change with scaling so the Gaussian and clustering would not be effected as much because the shape of the data did not change.\n",
    "    \n",
    "    lsvm uses the values the valuses of features in it's calculation, so with scaling it does not favour a particular feature because of the unit difference\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: Combine the above three models using hard vote in VotingClassifier. Use the same Scaled (normalized) data as step 2. \n",
    "\n",
    "    * Again, report your prediction result in the form of confusion matrix (this time, one matrix). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[196  16]\n",
      " [  4 353]]\n"
     ]
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "for indexes in kf.split(features):\n",
    "    train_index, test_index = indexes\n",
    "    \n",
    "    train_data = [features.iloc[train_index], labels.iloc[train_index].values.ravel()]\n",
    "    test_data = [features.iloc[test_index], labels.iloc[test_index].values.ravel()]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data[0])\n",
    "    train_data[0] = scaler.transform(train_data[0])\n",
    "    test_data[0] = scaler.transform(test_data[0])\n",
    "    \n",
    "    vc = VotingClassifier(estimators=[\n",
    "        ('lsvc', LinearSVC()), ('gnb', GaussianNB()), ('knc', KNeighborsClassifier())], voting='hard')\n",
    "    predictions.extend(vc.fit(*train_data).predict(test_data[0]))\n",
    "    actuals.extend(test_data[1])\n",
    "\n",
    "def numToName(vals):\n",
    "    return [data_bunch.target_names[v] for v in vals]\n",
    "\n",
    "\n",
    "\n",
    "print(confusion_matrix(numToName(actuals), numToName(predictions), labels=data_bunch.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is there any improvement using the combined model compared to step2 results? If so, explain where you see improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    The only improvement is in false negative. \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer\n",
    "'''\n",
    "    The only improvement is in false negative. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Regression (40 pts)\n",
    "\n",
    "In this question, you're asked to build a regression model to predict the price of a new house based on the data of previously sold houses that is provided in a dataset from California Housing Prices.\n",
    "\n",
    "#### Part A. Random Forest Regression  \n",
    "As a first step, train a simple RandomForestRegressor model by following these steps:\n",
    "\n",
    "* Load the data from here: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
    "* Split it into train (2/3) and test (1/3), with random_state=42. \n",
    "\n",
    "* Scale all the features (except label) to map them into [0,1] interval. \n",
    "* Use the scaled training set to build a basic RandomForestRegressor model with n_estimators=100 and random_state=0.\n",
    "* Report the score (R^2) of your model on train and test set. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that has not been mentioned.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8078342153254117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your solution\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "ch_data = fetch_california_housing()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ch_data.data, ch_data.target, random_state=42)\n",
    "scaler = MinMaxScaler(copy=True, feature_range=(0, 1)).fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "regr_raw = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "regr_raw.fit(X_train, y_train)\n",
    "\n",
    "regr_raw.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B. Improve the model\n",
    "\n",
    "Now try to improve the model by tuning the parameters:\n",
    "\n",
    "* Use GridSearchCV (with 5-fold cross-validation to make the train and validation sets) to tune part A's RandomForestRegressor's parameters as follows: \n",
    "    * max_features with 'auto', 'sqrt', and 'log2' \n",
    "    * max_depth with None, 10, 20, and 30\n",
    "    * min_samples_split with 2, 5, and 10\n",
    "\n",
    "* Report the tuned values of parameters as well as the score of the tuned model, on both train and test sets.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "Use the same scaled data and the same train, test split as Part A. Also for any unmentioned paramter, use the same default as Part A.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "regr = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "\n",
    "parameters = {'max_features':('auto', 'sqrt', \"log2\"), 'max_depth':[None, 10, 20, 30], \"min_samples_split\": [2, 5, 10]}\n",
    "gsCV = GridSearchCV(regr, parameters, cv=5).fit(X_train, y_train)\n",
    "gsCV.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C. Dimension Reduction\n",
    "\n",
    "Reduce the dimensionality of the data set to 4 dimensions, using PCA and RFE and repeat the predictions and report the scores. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "Make sure you use the same set up as Part B. That is the same train, test split, the same scaled data, and the same tuned values (i.e., you just take the one best tuned model and no need to do the corss-validation tuning again). \n",
    "</div>\n",
    "\n",
    "__PCA__:\n",
    "\n",
    "* Use PCA to reduce the features dimensions to 4 components. Note that knowing where and when PCA must be applied, is part of the question.\n",
    "* Now use the new feature set (4 PCA components rather than the original features) and report the R^2 score on the test set, using the same tuned  RandomForestRegressor model as part B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4).fit(X_train)\n",
    "\n",
    "regr_pca = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "regr_pca.fit(pca.transform(X_train), y_train)\n",
    "\n",
    "regr_pca.score(pca.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RFE__:\n",
    "\n",
    "* Use RFE with a simple linear regression model (using default values) to reduce the data to its 4 most informative features. Note that knowing how to properly apply RFE, is part of the question.\n",
    "* Now use the new feature set (top 4 features rather than the original features) and report the R^2 score on the test set, using the same tuned  RandomForestRegressor model as part B. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "regr_RFE = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "selector = RFE(regr_RFE, 4)\n",
    "selector = selector.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptually, what is the key difference between dimension reduction using RFE and PCA. \n",
    "* Explian your observation on the regresion results of RFE and PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Text Analysis (25 pts)\n",
    "\n",
    "In this question, you are given a dataset of movie_plots and asked to build an LDA (Latent Dirichlet Allocation) model to extract different topics that exist among these plots, by following these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Read the given \"movie_plots.csv\" file and load it to a pandas dataframe.\n",
    "- Vectorize the plots (provided in the \"Plot\" feature) using CountVectorizer, while considering the followings:\n",
    "    - use only bi-grams tokens. \n",
    "    - ignore english_stop_words.\n",
    "    - keep the maximum number of features at 10,000.\n",
    "    - drop features that appear in more than 5% of movies.\n",
    "    - drop features that appear in less than 10 movies.\n",
    "- Next build an LDA model (random_state=0) with 10 topics and fit it with all the vectorized movie plots.\n",
    "- Now print the topics, using the 5 most important words per topics\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    Use `default` values for all the parameters that have not been mentioned.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "movie_pd = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2), stop_words=\"english\", max_features=10000, max_df=0.5, min_df=10).fit(movie_pd[\"Plot\"])\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0).fit(cv.fit_transform(movie_pd[\"Plot\"]))\n",
    "    \n",
    "rev_dict = {v: k for k,v in cv.vocabulary_.items()}\n",
    "for i, topic in enumerate(lda.components_, start=1):\n",
    "    sorted_t = np.argsort(topic)[::-1][:5]\n",
    "    print('Top in '+str(i)+':', ' '.join(rev_dict[x] for x in sorted_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now add uni-gram and tri-gram tokens to the bi-gram tokens, and keep everything else the same and redo the LDA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        comedy\n",
       "1                       mystery\n",
       "2                        comedy\n",
       "3                        comedy\n",
       "4                        comedy\n",
       "                 ...           \n",
       "5955      crime, drama, mystery\n",
       "5956                    western\n",
       "5957    biography, crime, drama\n",
       "5958    biography, crime, drama\n",
       "5959                      drama\n",
       "Name: Genre, Length: 5960, dtype: category\n",
       "Categories (547, object): [usa, usa, can, action, action adventure, ..., western drama, western thriller, western, biography, western, romantic drama]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your solution\n",
    "cv = CountVectorizer(ngram_range=(1,3), stop_words=\"english\", max_features=10000, max_df=0.5, min_df=10).fit(movie_pd[\"Plot\"])\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0).fit(cv.fit_transform(movie_pd[\"Plot\"]))\n",
    "    \n",
    "rev_dict = {v: k for k,v in cv.vocabulary_.items()}\n",
    "for i, topic in enumerate(lda.components_, start=1):\n",
    "    sorted_t = np.argsort(topic)[::-1][:5]\n",
    "    print('Top in '+str(i)+':', ' '.join(rev_dict[x] for x in sorted_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - What major changes you see in the resulting topics?  Explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
