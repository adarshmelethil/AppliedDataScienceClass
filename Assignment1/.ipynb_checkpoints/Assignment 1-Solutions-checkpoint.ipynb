{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 1** - 100 marks\n",
    "\n",
    "**Due:** October 4th, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you were done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - The Zipf mystery (50 points)\n",
    "\n",
    "In this problem, we'd like to read the text from a book and perform some simple statistical analysis on the word counts. We have provided you with the actual text from [Lost On The Moon or, In Quest of the Field of Diamonds](https://www.goodreads.com/book/show/8636132-lost-on-the-moon-or-in-quest-of-the-field-of-diamonds) book in a file named 'the book.txt'. The file is cleaned up and only contains alphanumeric characters, i.e. no punctuation, quotation marks, etc.\n",
    "\n",
    "Read the file and break it down to its words. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        file_contents = f.read()\n",
    "        return file_contents.split()\n",
    "\n",
    "words = read_and_tokenize('the book.txt')\n",
    "words[1101:1111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sorted list of unique words in the book. Store the list in a variable called `V`. Also complete the `get_word_index` function below that gets a word and finds its index within `V`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [*sorted(set(words))]\n",
    "def get_word_index(word):\n",
    "    return V.index(word)\n",
    "\n",
    "get_word_index('water'), get_word_index('about')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using no loops, and by only using `map` and `filter` built-in python functions traverse through the `V` (vocabulary) list above to find:\n",
    "\n",
    "* `long_words`: The list of words that have 10 letters or more \n",
    "* `no_vowels`: A list of all words but with vowels (aoeiu) removed. You can nest `map` and `filter` calls to iterate through the characters of the words.\n",
    "\n",
    "(5+5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = [*filter(lambda w: len(w)>=10, V)]\n",
    "no_vowels = [*map(lambda w: ''.join(filter(lambda c: c not in 'aoeiu', w)), V)]\n",
    "\n",
    "long_words, no_vowels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy array of size `|V|` that only contains 0s. Store it in a variable named `frequencies`. Use this array to count the number of times each word has appeared in the book. For example `frequencies[9]` should store how many times the word located in the index 9 of `V` (the sorted list) --which is the word \"about\"-- has been appreaed in the book (165 times). (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frequencies = np.zeros(len(V))\n",
    "for word in words:\n",
    "    frequencies[get_word_index(word)] += 1\n",
    "    \n",
    "frequencies, frequencies[9]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word that appeared most frequently in the book. Find the word itself as well as the number of times it was repeated in the book. Use numpy functions, i.e. do not iterate over the `frequencies` array manually using a `for` loop. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.argmax(frequencies)\n",
    "max_frequency = frequencies[max_index]\n",
    "most_common_word = V[max_index]\n",
    "\n",
    "print(f'\"{most_common_word}\" is the most common word which has appeared {max_frequency} times in the book.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all frequency values by dividing them by the maximum frequency value (using vectorized operators). After this the most common word in the book should get a normalized frequency of `1` and all other words get some value \n",
    "between `1/MAX` and `1`. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_frequencies = frequencies / max_frequency\n",
    "normalized_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check if the normalized frequencies have any corelation to their ranks. If such correlation exists, the Zipf's law states that it is linear in a log-log space. Take the logarithm of normalized frequencies (as y values) and create a numpy array of the same size containing the rank of each word (as x values). For example if the frequencies array is `[0.1, 1, 0.01, 0.0001]` the x and y values will be `X = [2, 1, 3, 4] Y=[-1, 0, -2, -4]`. \n",
    "\n",
    "You might want to sort the normalized frequencies first to make the task easier. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(np.sort(normalized_frequencies)[::-1])\n",
    "y = np.arange(1, 1+len(V))  # This is only true if there are no two words that have occured the same number of times.\n",
    "# In the book there are many words that have appeared only once so this is not very accurate, \n",
    "# but it is still a good approximation. If you use `argsort` (or scipy.stats.rankdata) to get the true ranks the PCC\n",
    "# Value will be closer to -1, somewhere around -0.94."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) on this data. The result is expected to be close to -1. Define appropriate functions for the the statistical calculations as neccessary. Additionally, you can use `pearsonr` function from `scipy` package to check if the calculated value is definitely correct. Though if you get a value close enough to -1 you can almost be sure that your implementation is correct and this step won't be necessary. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcc(x, y):\n",
    "    xy_sum = np.dot(x, y)\n",
    "    x_sum = np.sum(x)\n",
    "    y_sum = np.sum(y)\n",
    "    x2_sum = np.sum(x**2)\n",
    "    y2_sum = np.sum(y**2)\n",
    "    \n",
    "    n = len(x)\n",
    "    \n",
    "    return (n*xy_sum - x_sum*y_sum) / np.sqrt((n*x2_sum - x_sum**2)*(n*y2_sum - y_sum**2))\n",
    "\n",
    "pcc(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Log processing (50 points)\n",
    "\n",
    "In this part of the assignment we are going to use regular expressions to mine data out of some webserver log files. Although these problems can be solved without use of RegExes, but for this assignment you need to use them.\n",
    "\n",
    "A sample web server log file is provided along with this problem. In each line of the file one event is recorded. For simplicity all of the events in this file have the same format and are of the same type. Each event contains an ip address, date and time of the event, http method (`GET` or `POST`), a url, HTTP version, HTTP response code (usually 200), the response size in bytes, and the device's user agent which contains information about the device such as the brand and the operating system.\n",
    "\n",
    "Since these logs have such a well defined format regular expressions are the prefect tool for breaking them down into parts and perform different analysis on them.\n",
    "\n",
    "**Please make sure that when you are asked to write a function that _return_s something, you are _return_ing that value, not just _print_ing it**\n",
    "\n",
    "We start off with a random log line and write python functions that use regular expressions to break it off to pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "l = '5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"'\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the ip address part of the log line using regular expressions. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip_address(l):\n",
    "    return re.search('(\\d{1,3}\\.){3}\\d{1,3}', l)[0]\n",
    "\n",
    "get_ip_address(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the HTTP method, url, response code, and response size and returns a tuple. Use regular expressions. The http method is either `POST` or `GET` and the response code is always a 3 digit integer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_http_info(l):\n",
    "    matches = re.findall(r'(POST|GET) ((/[^/ ]*)+).*HTTP/\\d\\.\\d\\D+(\\d{3})\\s*(\\d+)', l)\n",
    "    method, url, _, response, size = matches[0]\n",
    "    response, size = int(response), int(size)\n",
    "    return method, url, response, size\n",
    "\n",
    "get_http_info(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to break the date and time section apart and create a python datetime object based on that. Mind the time zone. convert the datetimes to MDT. Using `strptime` is a better solution in general, but for this assignment please stick to writing RegExes so you become more comfortable in writing and debugging them. (20 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from calendar import month_abbr\n",
    "\n",
    "MDT = timezone(timedelta(minutes=-6*60 + 0))\n",
    "\n",
    "def get_datetime(l):\n",
    "    matches = re.search(r'\\[(\\d+)/(\\w+)/(\\d+)((:\\d+){3}) ([+-]\\d{4})\\]', l)\n",
    "    \n",
    "    day = int(matches.group(1))\n",
    "    month = [*month_abbr].index(matches.group(2))\n",
    "    year = int(matches.group(3))\n",
    "    (h,m,s) = [int(_) for _ in matches.group(4).split(':')[1:]]\n",
    "    tz = matches.group(6)\n",
    "\n",
    "    tzoffset = int(tz[1:3]) * 60 + int(tz[3:5])\n",
    "    if tz.startswith(\"-\"):\n",
    "        tzoffset = -tzoffset\n",
    "    \n",
    "    original_datetime = datetime(year, month, day, h, m, s, tzinfo=timezone(timedelta(minutes=tzoffset)))\n",
    "    \n",
    "    return original_datetime.astimezone(MDT)\n",
    "    \n",
    "\n",
    "get_datetime(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file line by line and use the `get_datetime` and `get_http_info` functions above to calculate the used bandwidth of the server (the sum of all the response sizes) per hour. Use a `dict` or a `defaultdict`. (15 points)\n",
    "\n",
    "For example if there are 4 logs like:\n",
    "\n",
    "    Sep 4 14:20 .... 65bytes\n",
    "    Sep 4 14:35 .... 80bytes\n",
    "    Sep 4 15:01 .... 44bytes\n",
    "    Sep 5 18:20 .... 40bytes\n",
    "\n",
    "The result will be like:\n",
    "\n",
    "    Sep 4 14:00  145\n",
    "    Sep 4 15:00  44\n",
    "    Sep 5 18:00  40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = dict()\n",
    "\n",
    "with open('log.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        time = get_datetime(line)\n",
    "        http_info = get_http_info(line)\n",
    "        hour = time.replace(minute=0, second=0, microsecond=0)\n",
    "        bandwidths[hour] = bandwidths.get(hour, 0) + http_info[3]\n",
    "\n",
    "bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
