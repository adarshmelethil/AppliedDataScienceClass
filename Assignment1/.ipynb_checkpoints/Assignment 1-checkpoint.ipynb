{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 1** - 100 marks\n",
    "\n",
    "**Due:** October 4th, 04.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you were done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - The Zipf mystery (50 points)\n",
    "\n",
    "In this problem, we'd like to read the text from a book and perform some simple statistical analysis on the word counts. We have provided you with the actual text from [Lost On The Moon or, In Quest of the Field of Diamonds](https://www.goodreads.com/book/show/8636132-lost-on-the-moon-or-in-quest-of-the-field-of-diamonds) book in a file named 'the book.txt'. The file is cleaned up and only contains alphanumeric characters, i.e. no punctuation, quotation marks, etc.\n",
    "\n",
    "Read the file and break it down to its words. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def read_and_tokenize(file_name):\n",
    "    with open(file_name, \"r\") as fp:\n",
    "        return [w for w in re.split('\\n| ', fp.read()) if w]\n",
    "\n",
    "words = read_and_tokenize('the book.txt')\n",
    "\n",
    "words[1101:1111] # Expected: ['the', 'latter', 'picked', 'it', 'up', 'gazed', 'at', 'it', 'first', 'from']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a sorted list of unique words in the book. Store the list in a variable called `V`. Also complete the `get_word_index` function below that gets a word and finds its index within `V`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution goes here\n",
    "V = sorted(list(set(words)))\n",
    "def get_word_index(word):\n",
    "    return V.index(word)\n",
    "\n",
    "get_word_index('about')  # Expected: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using no loops, and by only using `map` and `filter` built-in python functions traverse through the `V` (vocabulary) list above to find:\n",
    "\n",
    "* `long_words`: The list of words that have 10 letters or more \n",
    "* `no_vowels`: A list of all words but with vowels (aoeiu) removed. You can nest `map` and `filter` calls to iterate through the characters of the words.\n",
    "\n",
    "(5+5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "import re\n",
    "long_words = list(filter(lambda x: len(x)>10, V))\n",
    "no_vowels = list(map(lambda x: re.sub('[aeiouAEIOU]', '', x), V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy array of size `|V|` that only contains 0s. Store it in a variable named `frequencies`. Use this array to count the number of times each word has appeared in the book. For example `frequencies[9]` should store how many times the word located in the index 9 of `V` (the sorted list) --which is the word \"about\"-- has been appreaed in the book (165 times). (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Your solution\n",
    "frequencies = np.array([words.count(w) for w in V], dtype=float)\n",
    "print(frequencies[9])\n",
    "# frequencies, frequencies[9] # Expected: array([ 1.,  1.,  1., ..., 11.,  1.,  1.]), 165.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word that appeared most frequently in the book. Find the word itself as well as the number of times it was repeated in the book. Use numpy functions, i.e. do not iterate over the `frequencies` array manually using a `for` loop. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\" is the most common word which has appeared 3237.0 times in the book.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Your solution \n",
    "most_common_word = V[np.argmax(frequencies)]\n",
    "max_frequency = frequencies.max()\n",
    "\n",
    "print(f'\"{most_common_word}\" is the most common word which has appeared {max_frequency} times in the book.')\n",
    "# Expected: \"the\" is the most common word which has appeared 3237 times in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all frequency values by dividing them by the maximum frequency value (using vectorized operators). After this the most common word in the book should get a normalized frequency of `1` and all other words get some value \n",
    "between `1/MAX` and `1`. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00030893, 0.00030893, 0.00030893, ..., 0.00339821, 0.00030893,\n",
       "       0.00030893])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your solution\n",
    "normalized_frequencies = frequencies / max_frequency\n",
    "normalized_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check if the normalized frequencies have any corelation to their ranks. If such correlation exists, the Zipf's law states that it is linear in a log-log space. Take the logarithm of normalized frequencies (as y values) and create a numpy array of the same size containing the rank of each word (as x values). For example if the frequencies array is `[0.1, 1, 0.01, 0.0001]` the x and y values will be `X = [2, 1, 3, 4] Y=[-1, 0, -2, -4]`. \n",
    "\n",
    "You might want to sort the normalized frequencies first to make the task easier. (2.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False False False]\n",
      "-2472698.2598525416 26338.932947746907 False False\n",
      "-0.86319890952679 0.00919470788965177\n"
     ]
    }
   ],
   "source": [
    "normalized_frequencies.sort()\n",
    "x = np.arange(len(normalized_frequencies), 0, -1)\n",
    "y = np.log10(normalized_frequencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) on this data. The result is expected to be close to -1. Define appropriate functions for the the statistical calculations as neccessary. Additionally, you can use `pearsonr` function from `scipy` package to check if the calculated value is definitely correct. Though if you get a value close enough to -1 you can almost be sure that your implementation is correct and this step won't be necessary. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8631989095267899\n",
      "-0.8631989095267899\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Your solution goes here\n",
    "def pcc(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    x_m = x - np.mean(x)\n",
    "    y_m = y - np.mean(y)\n",
    "    return np.sum(x_m * y_m) \\\n",
    "            / np.sqrt(np.sum(np.square(x_m)) * np.sum(np.square(y_m)))\n",
    "\n",
    "print(pcc(x, y))\n",
    "print(pearsonr(x, y)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Log processing (50 points)\n",
    "\n",
    "In this part of the assignment we are going to use regular expressions to mine data out of some webserver log files. Although these problems can be solved without use of RegExes, but for this assignment you need to use them.\n",
    "\n",
    "A sample web server log file is provided along with this problem. In each line of the file one event is recorded. For simplicity all of the events in this file have the same format and are of the same type. Each event contains an ip address, date and time of the event, http method (`GET` or `POST`), a url, HTTP version, HTTP response code (usually 200), the response size in bytes, and the device's user agent which contains information about the device such as the brand and the operating system.\n",
    "\n",
    "Since these logs have such a well defined format regular expressions are the prefect tool for breaking them down into parts and perform different analysis on them.\n",
    "\n",
    "**Please make sure that when you are asked to write a function that _return_s something, you are _return_ing that value, not just _print_ing it**\n",
    "\n",
    "We start off with a random log line and write python functions that use regular expressions to break it off to pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "l = '5.106.145.204 - - [04/Sep/2019:13:51:39 +0430] \"POST /v1/crash-report/incident/report/ HTTP/1.1\" 200 65 \"-\" \"Dalvik/1.6.0 (Linux; U; Android 4.2.2; GT-S7272 Build/JDQ39)\"'\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the ip address part of the log line using regular expressions. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.106.145.204'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ip_address(l):\n",
    "    # Your solution here\n",
    "    res = re.match(\"(\\d{1,3}\\.){3}\\d{1,3}\", l)\n",
    "    return res[0] if res else \"\"\n",
    "\n",
    "get_ip_address(l)  # Expected: '5.106.145.204'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that extracts the HTTP method, url, response code, and response size and returns a tuple. Use regular expressions. The http method is either `POST` or `GET` and the response code is always a 3 digit integer. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('POST', '/v1/crash-report/incident/report/ HTTP/1.1\"', 200, 65)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_http_info(l):\n",
    "    # Your solution here\n",
    "    http_method, url, code, retSize = re.match(\"^.*(POST|GET)\\s(\\/.*\\/?)\\s.*(\\d{3})\\s(\\d+)\\s.*$\", l).groups()\n",
    "    return http_method, url, int(code), int(retSize)\n",
    "\n",
    "print(get_http_info(l))  # Expected: ('POST', '/v1/crash-report/incident/report/', 200, 65)\n",
    "# Please note that the last two numbers are converted to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to break the date and time section apart and create a python datetime object based on that. Mind the time zone. convert the datetimes to MDT. Using `strptime` is a better solution in general, but for this assignment please stick to writing RegExes so you become more comfortable in writing and debugging them. (20 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 9, 4, 3, 21, 39, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from calendar import month_abbr\n",
    "\n",
    "MDT = timezone(timedelta(minutes=-6*60 + 0))\n",
    "\n",
    "def get_datetime(l):\n",
    "    day, month, year, hours, minutes, seconds, timezone_hr, timezone_min = re.match('^.*\\[(\\d{2})/(\\w{3})/(\\d{4}):(\\d{1,2}):(\\d{1,2}):(\\d{1,2}) ([\\-\\+]\\d{2})(\\d{2})\\].*$', l).groups()\n",
    "    day, year, hours, minutes, seconds = int(day), int(year), int(hours), int(minutes), int(seconds)\n",
    "    month = dict(zip(month_abbr, range(len(month_abbr))))[month]\n",
    "    dt_timezone = timezone(timedelta(minutes=int(timezone_hr)*60+int(timezone_min)))\n",
    "    \n",
    "    return datetime(year, month, day, hour=hours, minute=minutes, second=seconds, tzinfo=dt_timezone).astimezone(MDT)\n",
    "\n",
    "get_datetime(l)  # Expected: datetime.datetime(2019, 9, 4, 3, 21, 39, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=64800)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the log file line by line and use the `get_datetime` and `get_http_info` functions above to calculate the used bandwidth of the server (the sum of all the response sizes) per hour. Use a `dict` or a `defaultdict`. (15 points)\n",
    "\n",
    "For example if there are 4 logs like:\n",
    "\n",
    "    Sep 4 14:20 .... 65bytes\n",
    "    Sep 4 14:35 .... 80bytes\n",
    "    Sep 4 15:01 .... 44bytes\n",
    "    Sep 5 18:20 .... 40bytes\n",
    "\n",
    "The result will be like:\n",
    "\n",
    "    Sep 4 14:00  145\n",
    "    Sep 4 15:00  44\n",
    "    Sep 5 18:00  40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019, Jul 20 08:00 49454 bytes\n",
      "2019, Jul 20 09:00 40469 bytes\n",
      "2019, Jul 20 10:00 43556 bytes\n",
      "2019, Jul 20 11:00 82526 bytes\n",
      "2019, Jul 20 12:00 56328 bytes\n",
      "2019, Jul 20 13:00 98862 bytes\n",
      "2019, Jul 20 14:00 119679 bytes\n",
      "2019, Jul 20 15:00 57126 bytes\n",
      "2019, Jul 20 16:00 135680 bytes\n",
      "2019, Jul 20 17:00 48710 bytes\n",
      "2019, Jul 20 18:00 45631 bytes\n",
      "2019, Jul 20 19:00 9805 bytes\n",
      "2019, Jul 20 20:00 3569 bytes\n",
      "2019, Jul 20 21:00 36087 bytes\n",
      "2019, Jul 20 22:00 55406 bytes\n",
      "2019, Jul 20 23:00 68764 bytes\n",
      "2019, Jul 21 00:00 30101 bytes\n",
      "2019, Jul 21 01:00 83251 bytes\n",
      "2019, Jul 21 02:00 77896 bytes\n",
      "2019, Jul 21 03:00 65166 bytes\n",
      "2019, Jul 21 04:00 66326 bytes\n",
      "2019, Jul 21 05:00 86495 bytes\n",
      "2019, Jul 21 06:00 76888 bytes\n",
      "2019, Jul 21 07:00 65378 bytes\n",
      "2019, Jul 21 08:00 116337 bytes\n",
      "2019, Jul 21 09:00 55348 bytes\n",
      "2019, Jul 21 10:00 40975 bytes\n",
      "2019, Jul 21 11:00 51204 bytes\n",
      "2019, Jul 21 12:00 56527 bytes\n",
      "2019, Jul 21 13:00 50933 bytes\n",
      "2019, Jul 21 14:00 29773 bytes\n",
      "2019, Jul 21 15:00 80279 bytes\n",
      "2019, Jul 21 16:00 68270 bytes\n",
      "2019, Jul 21 17:00 12714 bytes\n",
      "2019, Jul 21 18:00 6414 bytes\n",
      "2019, Jul 21 19:00 9186 bytes\n",
      "2019, Jul 21 20:00 4356 bytes\n",
      "2019, Jul 21 21:00 17380 bytes\n",
      "2019, Jul 21 22:00 20251 bytes\n",
      "2019, Jul 21 23:00 25015 bytes\n",
      "2019, Jul 22 00:00 25326 bytes\n",
      "2019, Jul 22 01:00 21912 bytes\n",
      "2019, Jul 22 02:00 27967 bytes\n",
      "2019, Jul 22 03:00 56146 bytes\n",
      "2019, Jul 22 04:00 100773 bytes\n",
      "2019, Jul 22 05:00 87930 bytes\n",
      "2019, Jul 22 06:00 77608 bytes\n",
      "2019, Jul 22 07:00 84938 bytes\n",
      "2019, Jul 22 08:00 114074 bytes\n",
      "2019, Jul 22 09:00 139403 bytes\n",
      "2019, Jul 22 10:00 91115 bytes\n",
      "2019, Jul 22 11:00 85637 bytes\n",
      "2019, Jul 22 12:00 113985 bytes\n",
      "2019, Jul 22 13:00 160342 bytes\n",
      "2019, Jul 22 14:00 147641 bytes\n",
      "2019, Jul 22 15:00 120636 bytes\n",
      "2019, Jul 22 16:00 81082 bytes\n",
      "2019, Jul 22 17:00 66001 bytes\n",
      "2019, Jul 22 18:00 15379 bytes\n",
      "2019, Jul 22 19:00 17998 bytes\n",
      "2019, Jul 22 20:00 10826 bytes\n",
      "2019, Jul 22 21:00 24312 bytes\n",
      "2019, Jul 22 22:00 33147 bytes\n",
      "2019, Jul 22 23:00 24325 bytes\n",
      "2019, Jul 23 00:00 27712 bytes\n",
      "2019, Jul 23 01:00 63858 bytes\n",
      "2019, Jul 23 02:00 44863 bytes\n",
      "2019, Jul 23 03:00 28687 bytes\n",
      "2019, Jul 23 04:00 82434 bytes\n",
      "2019, Jul 23 05:00 91401 bytes\n",
      "2019, Jul 23 06:00 84796 bytes\n",
      "2019, Jul 23 07:00 72745 bytes\n",
      "2019, Jul 23 08:00 35005 bytes\n",
      "2019, Jul 23 09:00 72550 bytes\n",
      "2019, Jul 23 10:00 90913 bytes\n",
      "2019, Jul 23 11:00 86130 bytes\n",
      "2019, Jul 23 12:00 73650 bytes\n",
      "2019, Jul 23 13:00 58672 bytes\n",
      "2019, Jul 23 14:00 95681 bytes\n",
      "2019, Jul 23 15:00 72453 bytes\n",
      "2019, Jul 23 16:00 47086 bytes\n",
      "2019, Jul 23 17:00 7968 bytes\n",
      "2019, Jul 23 18:00 4677 bytes\n",
      "2019, Jul 23 19:00 10726 bytes\n",
      "2019, Jul 23 20:00 9506 bytes\n",
      "2019, Jul 23 21:00 6394 bytes\n",
      "2019, Jul 23 22:00 25995 bytes\n",
      "2019, Jul 23 23:00 64144 bytes\n",
      "2019, Jul 24 00:00 24542 bytes\n",
      "2019, Jul 24 01:00 29869 bytes\n",
      "2019, Jul 24 02:00 39192 bytes\n",
      "2019, Jul 24 03:00 44674 bytes\n",
      "2019, Jul 24 04:00 80986 bytes\n",
      "2019, Jul 24 05:00 50236 bytes\n",
      "2019, Jul 24 06:00 100834 bytes\n",
      "2019, Jul 24 07:00 57701 bytes\n",
      "2019, Jul 24 08:00 58621 bytes\n",
      "2019, Jul 24 09:00 75741 bytes\n",
      "2019, Jul 24 10:00 20267 bytes\n",
      "2019, Jul 24 11:00 48225 bytes\n",
      "2019, Jul 24 12:00 45402 bytes\n",
      "2019, Jul 24 13:00 52613 bytes\n",
      "2019, Jul 24 14:00 113918 bytes\n",
      "2019, Jul 24 15:00 58613 bytes\n",
      "2019, Jul 24 16:00 75026 bytes\n",
      "2019, Jul 24 17:00 107463 bytes\n",
      "2019, Jul 24 18:00 6480 bytes\n",
      "2019, Jul 24 19:00 3702 bytes\n",
      "2019, Jul 24 20:00 9946 bytes\n",
      "2019, Jul 24 21:00 11825 bytes\n",
      "2019, Jul 24 22:00 13964 bytes\n",
      "2019, Jul 24 23:00 15936 bytes\n",
      "2019, Jul 25 00:00 25976 bytes\n",
      "2019, Jul 25 01:00 38511 bytes\n",
      "2019, Jul 25 02:00 44458 bytes\n",
      "2019, Jul 25 03:00 50009 bytes\n",
      "2019, Jul 25 04:00 74315 bytes\n",
      "2019, Jul 25 05:00 48235 bytes\n",
      "2019, Jul 25 06:00 27275 bytes\n",
      "2019, Jul 25 07:00 38408 bytes\n",
      "2019, Jul 25 08:00 35241 bytes\n",
      "2019, Jul 25 09:00 42373 bytes\n",
      "2019, Jul 25 10:00 62963 bytes\n",
      "2019, Jul 25 11:00 28674 bytes\n",
      "2019, Jul 25 12:00 77992 bytes\n",
      "2019, Jul 25 13:00 45697 bytes\n",
      "2019, Jul 25 14:00 48120 bytes\n",
      "2019, Jul 25 15:00 16894 bytes\n",
      "2019, Jul 25 16:00 11946 bytes\n",
      "2019, Jul 25 17:00 9182 bytes\n",
      "2019, Jul 25 18:00 4174 bytes\n",
      "2019, Jul 25 19:00 5998 bytes\n",
      "2019, Jul 25 20:00 22558 bytes\n",
      "2019, Jul 25 21:00 4467 bytes\n",
      "2019, Jul 25 22:00 13011 bytes\n",
      "2019, Jul 25 23:00 15906 bytes\n",
      "2019, Jul 26 00:00 11325 bytes\n",
      "2019, Jul 26 01:00 47222 bytes\n",
      "2019, Jul 26 02:00 47935 bytes\n",
      "2019, Jul 26 03:00 40544 bytes\n",
      "2019, Jul 26 04:00 72866 bytes\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "logged_data = []\n",
    "with open(\"log.txt\", \"r\") as fp:\n",
    "    prev_dt = None\n",
    "    size_sum = 0\n",
    "    for _, line in enumerate(fp):\n",
    "        cur_dt = get_datetime(line)\n",
    "        method, url, code, r_size = get_http_info(line)\n",
    "        logged_data.append((cur_dt, r_size))\n",
    "        if prev_dt is None:\n",
    "            prev_dt = cur_dt\n",
    "            size_sum = r_size\n",
    "        \n",
    "#         print(prev_dt.date(), cur_dt.date(), prev_dt.date() == cur_dt.date())\n",
    "        if prev_dt.date() != cur_dt.date() or prev_dt.hour != cur_dt.hour:\n",
    "            print(\"{0} {1} bytes\".format(cur_dt.strftime(\"%Y, %b %d %H:00\"), size_sum))\n",
    "            prev_dt = cur_dt\n",
    "            size_sum = r_size\n",
    "        else:\n",
    "            size_sum += r_size\n",
    "#         if prev_dt:\n",
    "#             break\n",
    "\n",
    "# print(logged_data[:5])\n",
    "# print(len(logged_data))\n",
    "# No specific format for the output is expected\n",
    "# However the data will be something like:\n",
    "#  2019, 7, 20 07:00    49130 bytes\n",
    "#  2019, 7, 20 08:00    40469 bytes\n",
    "#  2019, 7, 20 09:00    43556 bytes\n",
    "#  2019, 7, 20 10:00    82526 bytes .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
