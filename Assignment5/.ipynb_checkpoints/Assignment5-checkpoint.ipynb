{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSF 519.01 Applied Data Science \n",
    "**Assignment 5** - 100 marks\n",
    "\n",
    "**Due:** November 25, 05.00 pm.\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE: each task must be implemented as asked, even if there are other easier or better solutions.**\n",
    "\n",
    "**How to deliver:**\n",
    "Edit this file and write your solutions in sections specified with `# Your solution`. Test your code and when you are done, submit this notebook as an `.ipynb` file to D2L dropbox. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam detection \n",
    "\n",
    "Here we have a dataset of text messages which are labeled as spam or ham. We want to read the dataset and use a clustering algorithm to tell spam messages from non-spam (ham!) ones. The data is in tsv format with two columns: label and text. TSV is just like csv but the column values are separated by a tab instead of a `,`. \n",
    "\n",
    "1. Read the file into a dataframe\n",
    "2. Convert `label` column to pandas categorical data type\n",
    "3. complete the `clean_text` function and apply it to the text column. To clean up:\n",
    "    1. Make it lowercase\n",
    "    2. Remove all of the punctuations (use `string.punctuation` and `str.translate`)\n",
    "    3. Replace repetetive whitespaces with just one blank charachter (e.g.: 'i    had \\tan apple' -> 'i had an apple')\n",
    "    4. Removing the stop words\n",
    "    5. Stem each word using snowball stemmer provided in `nltk`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "stemmer = ...\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    text: str, returns: str\n",
    "    \"\"\"\n",
    "    # Your solution\n",
    "    return ...\n",
    "\n",
    "\n",
    "sms = ...\n",
    "# Your solution\n",
    "print(sms.dtypes)  # Expected: label category text object dtype: object\n",
    "sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test sets (20% test 80% training), use `stratify` parameter to ensure that there is an even split for both categories. X values should be the vectors and y values the labels.\n",
    "\n",
    "\n",
    "Complete `build_tfidf` function below to make and fit a TF-IDF vectorizer with `min_df` = 2. This function is later called in the loop. \n",
    "\n",
    "Similar to previous assignments and problems, change the random state in a for loop from 0 to 9, in each iteration:\n",
    "\n",
    "1. The vectorizer instance is created on the training set. Convert the `X_train` and `X_test` by calling the `transform` method. You don't have to refit the transformer in the loop.\n",
    "2. train and evaluate these classifiers: \n",
    "\n",
    "    * LogisticRegression \n",
    "    * LinearSVC\n",
    "    * Naïve Bayes, with Bernoulli distribution\n",
    "    * Decision tree, use 20 for `min_samples_split` to prevent overfitting\n",
    "    * Random Forest, with a 100 estimators and use min_samples_split like above\n",
    "\n",
    " \n",
    "\n",
    "Use `random_state`s from in `[0, 5)` in classifier contruction for those which accept this parameter.\n",
    "Keep record of these scores in a pandas dataframe and make a boxplot to compare them. Set ylim to `(0.85, 1)`. \n",
    "\n",
    "**You should do all these inside the `fit_eval` and `build_tfidf` functions. Don't add any lines of code before or after it.** Also it doesn't need to return any values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def build_tfidf(X_train):\n",
    "    vectorizer = TfidfVectorizer(...)  # Your solution\n",
    "    # Fit the vectorizer here\n",
    "    return (vectorizer,)  # A tuple with only one item\n",
    "\n",
    "\n",
    "def fit_eval(get_vectorizer):\n",
    "    # Column names: 'Logistic Regression', 'Linear SVC', 'Naïve Bayes', 'Decision tree', 'Random Forest'\n",
    "    for split_seed in range(10):\n",
    "        X_train, X_test, y_train, y_test = ...\n",
    "        vectorizer = get_vectorizer(X_train)[0]\n",
    "        # Apply the transformations using `vectorizer` instance above\n",
    "        \n",
    "        # Your solution\n",
    "    \n",
    "    # Should not return anything\n",
    "\n",
    "fit_eval(build_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call `fit_eval` function again but this time instead of using the TF-IDF vectors directly, we make a new function that builds, fits, and returns: a LDA with 25 topics and use topic coverage vector of each document (text message). Use 0 as the `random_state` and a `CountVectorizer` with `min_df` = 2 to vectorize the text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Your solution\n",
    "def build_lda(X_train):\n",
    "    \"\"\"\n",
    "    Create a count vectorizer as described above, feed the vectors into an LDA and fit it. \n",
    "    It should return both the lda and count vectorizer as they're both going to be used later. \n",
    "    Just keep the return statement as is and you should be good to go.\n",
    "    \"\"\"\n",
    "    # Your solution, \n",
    "    count_vectorizer = ...\n",
    "    lda = ...\n",
    "    return lda, count_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit_eval(build_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `components_` attribute of the LDA to find top 5 words of each topic and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _,_,_ = train_test_split(sms['text'], sms['label'], random_state=0, stratify=...# The same as fit_eval)  \n",
    "lda, cv = build_lda(X_train)\n",
    "\n",
    "for i, topic in enumerate(lda.components_, start=1):\n",
    "    # Your solution\n",
    "    print('Top in '+str(i)+':', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a few sentences describe your observations with regard to comparing vanilla TF-IDF vs adding a LDA on top of a count vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit `enter` to edit this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
